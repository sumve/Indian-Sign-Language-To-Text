# Indian Sign Language (ISL) to Text Recognition

This project is a real-time **Sign Language to Text Conversion** system designed to improve accessibility for people who communicate using sign language. The system captures hand gestures through a webcam, recognizes the corresponding sign using deep learning & computer vision, and converts it into readable text. It also provides word suggestions and text-to-speech output to enhance usability.

The application is built as a desktop GUI and runs completely **offline**.The system is glove-free, runs on a standard webcam, and is optimized for **CPU-based real-time inference**.

---

## ğŸ” Overview

This project detects one or two hands using MediaPipe, extracts a dynamic Region of Interest (ROI), and classifies the gesture using a **MobileNetV2-based CNN** trained on a **self-curated ISL dataset**.  
Stability and cooldown logic are applied to ensure accurate and readable text output.

---

## âœ¨ Features

- **47-class gesture recognition: Images & Landmarks(.npy)**
  - Alphabets: Aâ€“Z  
  - Digits: 0â€“9  
  - Control gestures: `SPACE`, `DELETE`, `CLEAR`
- Real-time webcam inference
- Multi-hand landmark-driven ROI extraction
- Gesture stabilization using consecutive-frame validation
- Cooldown policy to avoid repeated predictions
- Desktop GUI using Tkinter
- Offline **Text-to-Speech (TTS)** output using pTTS
- Complete pipeline: data collection â†’ training â†’ inference
- Hybrid prediction pipeline utilising both images & numpy arrays of hand landmarks for better accuracy.
- Stable prediction logic to avoid flickering outputs
- Sentence formation from continuous gestures
- Word suggestions to assist faster text completion
- Delete (backspace) and Clear controls for easy correction
- 300 images per class with two distinct people & varied lighting conditions.

---

## ğŸ§  Technology Stack

- **OpenCV** â€“ Webcam capture and image processing  
- **MediaPipe Hands** â€“ 21-point hand landmark detection (multi-hand support)  
- **TensorFlow / Keras** â€“ Model training and inference  
- **MobileNetV2** â€“ Transfer learning backbone  
- **AdamW + Cosine Decay** â€“ Optimized training strategy  
- **scikit-learn** â€“ Class balancing  
- **Tkinter** â€“ Desktop GUI  
- **pTTS** â€“ Text-to-speech synthesis  

---

## ğŸ“Š Dataset

- Self-collected static-frame ISL dataset
- ~300 images per class (â‰ˆ 14100 images total)
- 80/20 trainingâ€“validation split
- Strong data augmentation:
  - Rotation, zoom, shear
  - Brightness variation
  - Horizontal flips
  - Normalization (1/255)

## Dataset Structure
```
dataSet/
â”œâ”€â”€ A/
â”œâ”€â”€ B/
â”œâ”€â”€ C/
â”œâ”€â”€ D/
â”œâ”€â”€ E/
â”œâ”€â”€ F/
â”œâ”€â”€ G/
â”œâ”€â”€ H/
â”œâ”€â”€ I/
â”œâ”€â”€ J/
â”œâ”€â”€ K/
â”œâ”€â”€ L/
â”œâ”€â”€ M/
â”œâ”€â”€ N/
â”œâ”€â”€ O/
â”œâ”€â”€ P/
â”œâ”€â”€ Q/
â”œâ”€â”€ R/
â”œâ”€â”€ S/
â”œâ”€â”€ T/
â”œâ”€â”€ U/
â”œâ”€â”€ V/
â”œâ”€â”€ W/
â”œâ”€â”€ X/
â”œâ”€â”€ Y/
â”œâ”€â”€ Z/
â”œâ”€â”€ 0/
â”œâ”€â”€ 1/
â”œâ”€â”€ 2/
â”œâ”€â”€ 3/
â”œâ”€â”€ 4/
â”œâ”€â”€ 5/
â”œâ”€â”€ 6/
â”œâ”€â”€ 7/
â”œâ”€â”€ 8/
â”œâ”€â”€ 9/
â”œâ”€â”€ SPACE/
â”œâ”€â”€ CLEAR/
â”œâ”€â”€ DELETE/
â”œâ”€â”€ Hello/
â”œâ”€â”€ Thankyou/
â”œâ”€â”€ Please/
â”œâ”€â”€ Sorry/
â”œâ”€â”€ Yes/
â”œâ”€â”€ No/
â”œâ”€â”€ Goodbye/
â””â”€â”€ I Love You/
```

## Usage

### 1. Collect Gesture Data

Run the dataset collection script:

python new_dataset.py

- Automatically creates class folders inside `DataSet/`
- Captures padded hand ROIs using MediaPipe landmarks
- Saves resized gesture images & landmarks (class_lm) for each class

---

### 2. Train the Model

Train the classifier using transfer learning:

python train_new.py
python landmark_train.py

Training details:
- Transfer learning using MobileNetV2
- Strong data augmentation
- Class-weight balancing
- Progressive fine-tuning
- Early stopping and model checkpointing
- Best model saved as `models/model.h5`
- Best landmark model saved as `models/landmark_model.h5`
---

### 3. Run Real-Time Inference

Start real-time sign recognition:

python gui.py

- Webcam feed is mirrored
- Unified ROI across detected hands
- Prediction accepted only after stability and cooldown checks
- Output text displayed live in the GUI

---

### 4. Text-to-Speech Output

- Click the **Speak** button in the GUI to hear the detected text using pTTS

## Results

- Validation accuracy: ~98â€“99%
- Real-time inference speed: ~18â€“25 FPS on CPU
- Stable predictions for static ISL gestures
- Stability and cooldown logic significantly reduce false positives

## Project Structure

A breakdown of the scripts and directories included in this project:

```text
.
â”œâ”€â”€ __pycache__/             # Python cache files
â”œâ”€â”€ DataSet/                 # Photos & Landmarks Dataset
â”œâ”€â”€ ISLData/                 # Indian Sign Language data (Kaggle)
â”œâ”€â”€ models/                  # Active/latest trained models
â”œâ”€â”€ models_old/              # Previous model iterations
â”œâ”€â”€ new/                     # Current development directory
â”‚   â”œâ”€â”€ check_lm.py          # Landmarks verification
â”‚   â”œâ”€â”€ gui.py               # Graphical User Interface implementation
â”‚   â”œâ”€â”€ hybrid_text.py       # Hybrid text processing logic
â”‚   â”œâ”€â”€ hybrid.py            # Main hybrid model logic
â”‚   â”œâ”€â”€ landmark_tra...      # Landmark training script
â”‚   â”œâ”€â”€ new_dataset....      # Dataset preprocessing script
â”‚   â”œâ”€â”€ train_new.py         # Updated training pipeline
â”œâ”€â”€ old/                     # Legacy code/scripts
â”œâ”€â”€ venv/                    # Primary virtual environment
â”œâ”€â”€ venv2/                   # Alternative/testing virtual environment
â”œâ”€â”€ .gitignore               # Files excluded from Git tracking
â””â”€â”€ README.md                # Project documentation
```

## Future Improvements

- Expand the dataset to 10k+ images per class
- Add temporal modeling for dynamic and transition-based gestures
- Improve two-hand coordinated gesture recognition
- Optimize the model using quantization and pruning for mobile deployment
- Extend the system to full ISL sentence-level translation
- Integrate speech-to-sign and sign-to-speech bidirectional support

## License

This project is intended for academic, research, and portfolio use.

## Citation

If you use this project for research, coursework, or benchmarking, please reference this repository and include the gesture label mapping provided in `models/classes.txt` for reproducibility.
