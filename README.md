# Indian Sign Language (ISL) to Text Recognition

A real-time **Indian Sign Language (ISL) to Text** recognition system that translates hand gestures into readable text using computer vision and deep learning.  
The system is **glove-free**, runs on a standard webcam, and is optimized for **CPU-based real-time inference**.

---

## ğŸ” Overview

This project detects one or two hands using MediaPipe, extracts a dynamic Region of Interest (ROI), and classifies the gesture using a **MobileNetV2-based CNN** trained on a **self-curated ISL dataset**.  
Stability and cooldown logic are applied to ensure accurate and readable text output.

---

## âœ¨ Features

- **49-class gesture recognition**
  - Alphabets: Aâ€“Z  
  - Digits: 0â€“9  
  - Control gestures: `SPACE`, `DELETE`, `CLEAR`, `DONE`, `NEXT`
- Real-time webcam inference
- Multi-hand landmark-driven ROI extraction
- Gesture stabilization using consecutive-frame validation
- Cooldown policy to avoid repeated predictions
- Desktop GUI using Tkinter
- Optional **Text-to-Speech (TTS)** output using gTTS
- Complete pipeline: data collection â†’ training â†’ inference

---

## ğŸ§  Technology Stack

- **OpenCV** â€“ Webcam capture and image processing  
- **MediaPipe Hands** â€“ 21-point hand landmark detection (multi-hand support)  
- **TensorFlow / Keras** â€“ Model training and inference  
- **MobileNetV2** â€“ Transfer learning backbone  
- **AdamW + Cosine Decay** â€“ Optimized training strategy  
- **scikit-learn** â€“ Class balancing  
- **Tkinter** â€“ Desktop GUI  
- **gTTS** â€“ Text-to-speech synthesis  

---

## ğŸ“Š Dataset

- Self-collected static-frame ISL dataset
- ~100 images per class (â‰ˆ 4900 images total)
- 80/20 trainingâ€“validation split
- Strong data augmentation:
  - Rotation, zoom, shear
  - Brightness variation
  - Horizontal flips
  - Normalization (1/255)

## Dataset Structure
```
dataSet/
â”œâ”€â”€ A/
â”œâ”€â”€ B/
â”œâ”€â”€ C/
â”œâ”€â”€ D/
â”œâ”€â”€ E/
â”œâ”€â”€ F/
â”œâ”€â”€ G/
â”œâ”€â”€ H/
â”œâ”€â”€ I/
â”œâ”€â”€ J/
â”œâ”€â”€ K/
â”œâ”€â”€ L/
â”œâ”€â”€ M/
â”œâ”€â”€ N/
â”œâ”€â”€ O/
â”œâ”€â”€ P/
â”œâ”€â”€ Q/
â”œâ”€â”€ R/
â”œâ”€â”€ S/
â”œâ”€â”€ T/
â”œâ”€â”€ U/
â”œâ”€â”€ V/
â”œâ”€â”€ W/
â”œâ”€â”€ X/
â”œâ”€â”€ Y/
â”œâ”€â”€ Z/
â”œâ”€â”€ 0/
â”œâ”€â”€ 1/
â”œâ”€â”€ 2/
â”œâ”€â”€ 3/
â”œâ”€â”€ 4/
â”œâ”€â”€ 5/
â”œâ”€â”€ 6/
â”œâ”€â”€ 7/
â”œâ”€â”€ 8/
â”œâ”€â”€ 9/
â”œâ”€â”€ SPACE/
â”œâ”€â”€ CLEAR/
â”œâ”€â”€ DELETE/
â”œâ”€â”€ DONE/
â”œâ”€â”€ NEXT/
â”œâ”€â”€ Hello/
â”œâ”€â”€ Thankyou/
â”œâ”€â”€ Please/
â”œâ”€â”€ Sorry/
â”œâ”€â”€ Yes/
â”œâ”€â”€ No/
â””â”€â”€ ILY/
```

## Usage

### 1. Collect Gesture Data

Run the dataset collection script:

python collect_data.py

- Automatically creates class folders inside `dataSet/`
- Captures padded hand ROIs using MediaPipe landmarks
- Saves resized gesture images for each class

---

### 2. Train the Model

Train the classifier using transfer learning:

python train.py

Training details:
- Transfer learning using MobileNetV2
- Strong data augmentation
- Class-weight balancing
- Progressive fine-tuning
- Early stopping and model checkpointing
- Best model saved as `models/model.h5`

---

### 3. Run Real-Time Inference

Start real-time sign recognition:

python webcam.py

- Webcam feed is mirrored
- Unified ROI across detected hands
- Prediction accepted only after stability and cooldown checks
- Output text displayed live in the GUI

---

### 4. Text-to-Speech Output

- Click the **Play Sound** button in the GUI to hear the detected text using TTS

## Results

- Validation accuracy: ~98â€“99%
- Real-time inference speed: ~18â€“25 FPS on CPU
- Stable predictions for static ISL gestures
- Majority of errors occur between visually similar hand shapes
- Stability and cooldown logic significantly reduce false positives

## Project Structure

A breakdown of the scripts and directories included in this project:

```text
.
â”œâ”€â”€ collect_data.py       # Dataset collection script
â”œâ”€â”€ train.py              # Model training script
â”œâ”€â”€ webcam.py             # Real-time inference + GUI
â”œâ”€â”€ plot.py               # Training curves visualization
â”œâ”€â”€ layers.py             # Model architecture inspection
â”œâ”€â”€ count.py              # Dataset class distribution check
â”œâ”€â”€ folders.py            # Dataset folder initialization
â”œâ”€â”€ models/               # Directory for saved models
â”‚   â”œâ”€â”€ model.h5          # Trained model weights
â”‚   â””â”€â”€ classes.txt       # Class label mapping
â”œâ”€â”€ dataSet/              # Gesture dataset (Aâ€“Z, 0â€“9, control gestures)
â””â”€â”€ README.md             # Project documentation
```

## Future Improvements

- Expand the dataset to 10k+ images per class
- Add temporal modeling for dynamic and transition-based gestures
- Improve two-hand coordinated gesture recognition
- Optimize the model using quantization and pruning for mobile deployment
- Extend the system to full ISL sentence-level translation
- Integrate speech-to-sign and sign-to-speech bidirectional support

## License

This project is intended for academic, research, and portfolio use.

## Citation

If you use this project for research, coursework, or benchmarking, please reference this repository and include the gesture label mapping provided in `models/classes.txt` for reproducibility.
